{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tiktoken version: 0.7.0\n",
      "torch version: 2.2.2\n"
     ]
    }
   ],
   "source": [
    "from importlib.metadata import version\n",
    "\n",
    "print('tiktoken version:', version('tiktoken'))\n",
    "print('torch version:', version('torch'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizing Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib.request\n",
    "\n",
    "if not os.path.exists('data/t8.shakespeare.txt'):\n",
    "    url = (\"https://ocw.mit.edu/ans7870/6/6.006/s08/lecturenotes/files/t8.shakespeare.txt\")\n",
    "    file_path = 'data/t8.shakespeare.txt'\n",
    "    urllib.request.urlretrieve(url, file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of characters: 5458199\n",
      "First 100 characters: This is the 100th Etext file presented by Project Gutenberg, and\n",
      "is presented in cooperation with Wo\n"
     ]
    }
   ],
   "source": [
    "with open('data/t8.shakespeare.txt', 'r') as file:\n",
    "    text = file.read()\n",
    "    \n",
    "\n",
    "print('Total number of characters:', len(text))\n",
    "print('First 100 characters:', text[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The goal of tokenization is to split a text into words, phrases, symbols, or other meaningful elements, which are called tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'is', 'the', '100th', 'Etext', 'file', 'presented', 'by', 'Project', 'Gutenberg', ',', 'and', 'is', 'presented', 'in', 'cooperation', 'with', 'World', 'Library', ',', 'Inc', '.', ',', 'from', 'their', 'Library', 'of', 'the', 'Future', 'and']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# remove , . \\n and convert to lowercase\n",
    "preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
    "preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "print(preprocessed[:30])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of words: 1158046\n"
     ]
    }
   ],
   "source": [
    "print('Total number of words:', len(preprocessed))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting token into token IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 34510\n"
     ]
    }
   ],
   "source": [
    "all_words = sorted(set(preprocessed))\n",
    "vocab_size = len(all_words)\n",
    "print('Vocabulary size:', vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = {token: idx for idx, token in enumerate(all_words)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: !\n",
      "1: \"\n",
      "2: #100]\n",
      "3: &\n",
      "4: &C\n",
      "5: &c\n",
      "6: '\n",
      "7: (\n",
      "8: )\n",
      "9: *\n",
      "10: ****\n",
      "11: *****\n",
      "12: ******\n",
      "13: ******This\n",
      "14: **Etexts\n",
      "15: **Information\n",
      "16: **Welcome\n",
      "17: *ANY*\n",
      "18: *EITHER*\n",
      "19: *Project\n",
      "20: *These\n",
      "21: *This\n",
      "22: *WANT*\n",
      "23: *not*\n",
      "24: ,\n",
      "25: -\n",
      "26: --\n",
      "27: -And\n",
      "28: -Break\n",
      "29: -But\n",
      "30: -Cheerly\n",
      "31: -Come\n",
      "32: -Give\n",
      "33: -Go\n",
      "34: -Here\n",
      "35: -Ho\n",
      "36: -How\n",
      "37: -I\n",
      "38: -Let\n",
      "39: -Luce\n",
      "40: -My\n",
      "41: -Nay\n",
      "42: -Now\n",
      "43: -Out\n",
      "44: -Please\n",
      "45: -Prithee\n",
      "46: -Spare\n",
      "47: -Still\n",
      "48: -THE\n",
      "49: -That\n",
      "50: -There\n",
      "51: -Thick\n",
      "52: -What\n",
      "53: -Where\n",
      "54: -Why\n",
      "55: -You\n",
      "56: -[Aside\n",
      "57: -a\n",
      "58: -and\n",
      "59: -even\n",
      "60: -from\n",
      "61: -give\n",
      "62: -gold\n",
      "63: -have\n",
      "64: -here\n",
      "65: -hey\n",
      "66: -hissing\n",
      "67: -if\n",
      "68: -in\n",
      "69: -is\n",
      "70: -marry\n",
      "71: -matter\n",
      "72: -me\n",
      "73: -nay\n",
      "74: -no\n",
      "75: -odours\n",
      "76: -of\n",
      "77: -on\n",
      "78: -out\n",
      "79: -perhaps-\n",
      "80: -say\n",
      "81: -take\n",
      "82: -the\n",
      "83: -there\n",
      "84: -they\n",
      "85: -thou\n",
      "86: -to\n",
      "87: -we\n",
      "88: -well\n",
      "89: -whose\n",
      "90: -why\n",
      "91: -yet\n",
      "92: .\n",
      "93: /\n",
      "94: 000\n",
      "95: 000=Trillion]\n",
      "96: 08\n",
      "97: 0INDEX\n",
      "98: 1\n",
      "99: 1-800-443-0238\n"
     ]
    }
   ],
   "source": [
    "for i, token in enumerate(all_words):\n",
    "    if i >= 100:\n",
    "        break\n",
    "    print(f'{i}: {token}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTokenizerV1:\n",
    "    def __init__(self, vocab):\n",
    "        self.str_to_int = vocab\n",
    "        self.int_to_str = {idx: token for token, idx in vocab.items()}\n",
    "        \n",
    "    def encode(self, text):\n",
    "        preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
    "                                \n",
    "        preprocessed = [\n",
    "            item.strip() for item in preprocessed if item.strip()\n",
    "        ]\n",
    "        return [self.str_to_int[token] for token in preprocessed]\n",
    "    \n",
    "    def decode(self, tokens):\n",
    "        return ''.join([self.int_to_str[idx] for idx in tokens])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8413, 11319, 24479, 24121, 31544, 11319]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = SimpleTokenizerV1(vocab)\n",
    "\n",
    "text = \"To be or not to be\"\n",
    "\n",
    "ids = tokenizer.encode(text)\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Tobeornottobe'"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "tokenizer.decode(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
